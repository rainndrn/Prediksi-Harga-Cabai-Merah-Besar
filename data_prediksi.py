# -*- coding: utf-8 -*-
"""Data Prediksi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q9cF4MgjlyKG-Q7QMT4oFzvt-UYUStgW

# **2. Data Understanding**
"""

import pandas as pd

# ============================
# 1. LOAD DATA
# ============================
df = pd.read_csv("/content/drive/MyDrive/Harga Bahan Pokok 2023 - 2024.csv", sep=";")

print("=== 1. SAMPLE DATA (10 baris pertama) ===")
display(df.head(10))

# ============================
# 2. INFORMASI DATASET
# ============================
print("=== 2. INFO DATA ===")
df.info()

# ============================
# 3. CEK MISSING VALUES
# ============================
print("\n=== 3. MISSING VALUES ===")
print(df.isna().sum())

# ============================
# 4. STATISTIK DESKRIPTIF
# ============================
print("\n=== 4. STATISTIK DESKRIPTIF ===")
display(df.describe(include="all"))

# ============================
# 5. UNIQUE VALUES PER KOLOM (opsional tapi bagus)
# ============================
print("\n=== 5. UNIQUE VALUES PER KOLOM ===")
for col in df.columns:
    print(f"{col}: {df[col].nunique()} unique values")

# ============================
# 6. CEK RANGE TANGGAL
# ============================
df["tanggal_lengkap"] = pd.to_datetime(df["tanggal_lengkap"], errors="coerce")

print("\n=== 6. RANGE TANGGAL ===")
print("Tanggal paling awal :", df["tanggal_lengkap"].min())
print("Tanggal paling akhir:", df["tanggal_lengkap"].max())

# ============================
# 7. CEK DATA KHUSUS KABUPATEN BEKASI
# ============================
print("\n=== 7. CEK FILTER LOKASI ===")
print(df["nama_kabupaten_kota"].value_counts())

# ============================
# 8. CEK OUTLIER UNTUK HARGA CABAI RAWIT MERAH
# (jika kolomnya memang punya harga)
# ============================
if "cabai_rawit_merah" in df.columns:
    print("\n=== 8. OUTLIERS CABAI RAWIT MERAH (IQR) ===")
    Q1 = df["cabai_rawit_merah"].quantile(0.25)
    Q3 = df["cabai_rawit_merah"].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5*IQR
    upper = Q3 + 1.5*IQR
    print("Batas bawah:", lower)
    print("Batas atas :", upper)
    print("Jumlah outlier:", df[(df["cabai_rawit_merah"] < lower) | (df["cabai_rawit_merah"] > upper)].shape[0])

"""# **3. Data Preparation**"""

import pandas as pd
import matplotlib.pyplot as plt

# ============================
# 1. LOAD DATASET
# ============================
df = pd.read_csv("/content/drive/MyDrive/Harga Bahan Pokok 2023 - 2024.csv", sep=";")

# ============================
# 2. FILTER HANYA KABUPATEN BEKASI
# ============================
df = df[df["nama_kabupaten_kota"] == "KABUPATEN BEKASI"]

print("Jumlah data setelah filter Bekasi:", len(df))

# ============================
# 3. PILIH KOMODITAS TARGET
# ganti sesuai kebutuhan (misal: 'bawang_merah', 'beras_premium', dll)
# ============================
target = "cabe_merah_besar"

if target not in df.columns:
    raise ValueError(f"Kolom {target} tidak ditemukan di dataset!")

print("Target yang dipilih:", target)

# ============================
# 4. KONVERSI TANGGAL & SET INDEX
# ============================
df["tanggal_lengkap"] = pd.to_datetime(df["tanggal_lengkap"], errors="coerce")
df = df.set_index("tanggal_lengkap")
df = df.sort_index()

# ============================
# 5. SELECT ONLY TARGET COLUMN
# ============================
ts = df[[target]]

print("\nData time series siap digunakan:")
display(ts.head())

# ============================
# 6. CEK MISSING VALUES & HANDLE
# ============================
print("\nMissing values sebelum imputasi:", ts.isna().sum())

# Cara imputasi (pilih salah satu):
ts[target] = ts[target].interpolate(method='linear')

print("Missing values setelah imputasi:", ts.isna().sum())

# ============================
# 7. RESAMPLING (opsional jika data tidak lengkap)
# Pastikan data daily
# ============================
ts = ts.asfreq("D", method="pad")

print("\nJumlah data setelah resampling:", ts.shape)

# ============================
# 8. VISUALISASI TIME SERIES
# ============================
plt.figure(figsize=(12,5))
plt.plot(ts[target])
plt.title(f"Trend Harga Harian: {target}")
plt.xlabel("Tanggal")
plt.ylabel("Harga")
plt.grid(True)
plt.show()

"""# **4. SARIMA**"""

# ============================
# 1. IMPORT LIBRARY
# ============================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Data time series yang sudah disiapkan sebelumnya
series = ts["cabe_merah_besar"]

# ============================
# 2. ADF TEST
# ============================
result = adfuller(series.dropna())

print("ADF Statistic :", result[0])
print("p-value       :", result[1])

if result[1] <= 0.05:
    print("\n✔ Data sudah stasioner")
else:
    print("\n✖ Data belum stasioner, perlu differencing")

# ============================
# 3. Plot ACF & PACF
# ============================
plt.figure(figsize=(14,6))

plt.subplot(1,2,1)
plot_acf(series.dropna(), ax=plt.gca(), lags=30)
plt.title("ACF")

plt.subplot(1,2,2)
plot_pacf(series.dropna(), ax=plt.gca(), lags=30, method='ywm')
plt.title("PACF")

plt.show()

# PARAMETER SARIMA
order = (1, 1, 1)
seasonal_order = (1, 1, 1, 7)

# ============================
# 5. FIT MODEL SARIMA
# ============================
model = SARIMAX(series, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)
model_fit = model.fit(disp=False)

print(model_fit.summary())

# ============================
# 6. DIAGNOSTIC CHECK
# ============================
model_fit.plot_diagnostics(figsize=(12,8))
plt.show()

# ============================
# 7. FORECAST 30 HARI
# ============================
forecast_steps = 30
forecast = model_fit.forecast(steps=forecast_steps)

plt.figure(figsize=(12,5))
plt.plot(series, label="Actual")
plt.plot(forecast, label="Forecast", linestyle="--")
plt.title("Forecast Harga Cabai Rawit Merah")
plt.legend()
plt.show()

# ============================
# 8. EVALUASI SARIMA
# ============================
train_size = int(len(series) * 0.8)
train, test = series[:train_size], series[train_size:]

model_eval = SARIMAX(train, order=order, seasonal_order=seasonal_order)
model_eval_fit = model_eval.fit(disp=False)

pred = model_eval_fit.forecast(steps=len(test))

# Hitung error
mae = mean_absolute_error(test, pred)
rmse = np.sqrt(mean_squared_error(test, pred))
mape = np.mean(np.abs((test - pred) / test)) * 100

print("MAE :", mae)
print("RMSE:", rmse)
print("MAPE:", mape, "%")

# Plot
plt.figure(figsize=(12,5))
plt.plot(train, label="Train")
plt.plot(test, label="Actual")
plt.plot(pred, label="Prediction", linestyle="--")
plt.legend()
plt.show()

"""# **4. LSTM**

1. Import Library
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.metrics import mean_absolute_error, mean_squared_error

"""2. Ambil Data Time Series"""

data = ts["cabe_merah_besar"].values.reshape(-1, 1)

print("\n=== 5 DATA AWAL KOMODITAS TERPILIH ===")
print(data[:5])

"""3. Normalisasi data (0-1)"""

scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

print("\n=== DATA SETELAH NORMALISASI (0-1) ===")
print(scaled_data[:5])

"""4. Membuat Sequence (Windowing)"""

def create_dataset(dataset, step):
    X, y = [], []
    for i in range(len(dataset) - step):
        X.append(dataset[i:i+step])
        y.append(dataset[i+step])
    return np.array(X), np.array(y)

time_step = 30
X, y = create_dataset(scaled_data, time_step)

print("\n=== BENTUK SEQUENCE LSTM ===")
print("X shape :", X.shape)
print("y shape :", y.shape)

"""5. Split Data Train/Test"""

train_size = int(len(X) * 0.8)

X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Ubah ke bentuk 3D untuk LSTM
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test  = X_test.reshape(X_test.shape[0],  X_test.shape[1],  1)

print("\n=== SHAPE DATASET UNTUK LSTM (3D) ===")
print("X_train:", X_train.shape)
print("X_test :", X_test.shape)

"""6. Bangun Model LSTM"""

model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(time_step, 1)),
    LSTM(32, return_sequences=False),
    Dense(16, activation='relu'),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse')
model.summary()

"""7. Train Model"""

history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.1,
    verbose=1
)

"""8. Prediksi & Inverse Scaling"""

predicted = model.predict(X_test)
predicted = scaler.inverse_transform(predicted)
actual = scaler.inverse_transform(y_test)

print("\n=== 10 DATA HASIL PREDIKSI vs ACTUAL ===")
print(pd.DataFrame({
    "Actual": actual.flatten()[:10],
    "Predicted": predicted.flatten()[:10]
}))

"""9. Evaluasi (MAE, RMSE, MAPE)"""

mae = mean_absolute_error(actual, predicted)
rmse = np.sqrt(mean_squared_error(actual, predicted))
mape = np.mean(np.abs((actual - predicted) / actual)) * 100

print("MAE :", mae)
print("RMSE:", rmse)
print("MAPE:", mape, "%")

"""10. Visualisasi Prediksi"""

plt.figure(figsize=(12,5))
plt.plot(actual, label="Actual", linewidth=2)
plt.plot(predicted, label="Predicted (LSTM)", linestyle="--")
plt.title("Prediksi Harga Cabai Rawit Merah - LSTM")
plt.legend()
plt.show()

import joblib
from statsmodels.tsa.statespace.sarimax import SARIMAX

# series: time series yang sudah diproses
model = SARIMAX(series, order=(1,1,1), seasonal_order=(1,1,1,7))
fitted = model.fit(disp=False)

joblib.dump(fitted, "sarima.pkl")
print("SARIMA model saved!")

from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import joblib
import numpy as np

# Series values
values = series.values.reshape(-1,1)

# Scaling
scaler = MinMaxScaler()
scaled = scaler.fit_transform(values)

# Windowing
def create_dataset(dataset, time_step=30):
    X, y = [], []
    for i in range(len(dataset)-time_step):
        X.append(dataset[i:i+time_step])
        y.append(dataset[i+time_step])
    return np.array(X), np.array(y)

time_step = 30
X, y = create_dataset(scaled, time_step)
X = X.reshape((X.shape[0], X.shape[1], 1))

# Split
train_size = int(len(X) * 0.8)
X_train, y_train = X[:train_size], y[:train_size]
X_test, y_test = X[train_size:], y[train_size:]

# Model
model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(time_step,1)),
    LSTM(32),
    Dense(16, activation="relu"),
    Dense(1)
])

model.compile(optimizer="adam", loss="mse")
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)

# Save model and scaler
model.save("lstm.h5")
joblib.dump(scaler, "scaler.pkl")

print("LSTM model saved!")